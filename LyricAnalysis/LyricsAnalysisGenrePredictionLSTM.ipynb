{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bxX4QrcHj-g"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "db9fGvd1Hj-k",
    "outputId": "682d9069-ea20-40e9-9934-b8bd83fe447b"
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'numpy', 'pandas', 'torch', 'wordcloud', 'transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "print(missing)\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import transformers\n",
    "\n",
    "from spacy.lang.en import English\n",
    "!python -m spacy download en_core_web_md\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "en = English()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oWWOExj1Hj-o",
    "outputId": "dadb2f7e-4d76-4c21-8ecd-849f69d7a746"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will set the device on which to train\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PvGZHgeHj-r"
   },
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)&(not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUCJOdFMHj-v"
   },
   "outputs": [],
   "source": [
    "\n",
    "STOPLIST = spacy.lang.en.stop_words.STOP_WORDS\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"_ _\",\"--\",\"...\", \"”\", \"”\"]\n",
    "\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "   def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "   def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "px5TnaQgHj-y"
   },
   "outputs": [],
   "source": [
    "artistsDF = pd.read_csv('artists-data.csv')\n",
    "artistsDF = artistsDF.drop_duplicates(subset = 'Link', keep ='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCM49DauHj-7"
   },
   "outputs": [],
   "source": [
    "lyricsDF = pd.read_csv('lyrics-data.csv')\n",
    "lyricsDF.rename(columns={'ALink':'Link'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3NEG8ESHj-_"
   },
   "outputs": [],
   "source": [
    "artistLyricsDF = pd.merge(lyricsDF,artistsDF, on='Link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ1FnDqXHj_E"
   },
   "outputs": [],
   "source": [
    "aldf = artistLyricsDF[['Artist', 'SName', 'Genre','Lyric', 'Popularity','Idiom']]\n",
    "finalDF = aldf[aldf.Lyric.notnull()]\n",
    "\n",
    "artists = finalDF['Artist'].unique()\n",
    "genres = finalDF['Genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "Ug_ig6DlHj_J",
    "outputId": "eefe9530-7d62-4c13-ff4f-5b0b840727f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.countplot(x='Genre', data=finalDF)\n",
    "\n",
    "plt.title('Song Count by Genre in All Languages')\n",
    "plt.ylabel('Number of Songs', fontsize=12)\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2T-5Cl6Hj_M"
   },
   "outputs": [],
   "source": [
    "englishDF = finalDF[finalDF['Idiom'] == 'ENGLISH']\n",
    "genreLabels = []\n",
    "\n",
    "for x in englishDF['Genre'].tolist():\n",
    "    if x == 'Rock':\n",
    "        y = True\n",
    "    else:\n",
    "        y = False\n",
    "    genreLabels.append(y)\n",
    "\n",
    "is_rock = np.array(genreLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fx-YIQ_2Hj_O"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "englishArtists = englishDF['Artist'].unique()\n",
    "englishGenres = englishDF['Genre'].unique()\n",
    "\n",
    "df = pd.DataFrame(columns=('artist', 'words', 'songcount'))\n",
    "j=0\n",
    "\n",
    "tokens =[]\n",
    "for artist in englishArtists:\n",
    "    num_words = 0\n",
    "    lyrics = englishDF['Lyric'][englishDF['Artist'] == artist]\n",
    "    #print(\"\\nArtist %s\" %artist, \"Num Lyrics %s\" %len(lyrics))\n",
    "    for lyric in lyrics:\n",
    "        #print(\"\\nArtist %s\" %artist,\"Lyric:\\n\" , lyric)\n",
    "        words = simple_tokenizer(lyric)\n",
    "        tokens.append(words)\n",
    "        num_words = num_words + len(words)\n",
    "    df.loc[j] = (artist, num_words, len(lyrics))\n",
    "    j+=1\n",
    "    i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "mplYIVaRHj_S",
    "outputId": "02cac09a-d91c-4325-98cb-5ec993862720"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.countplot(x='Genre', data=englishDF)\n",
    "\n",
    "plt.title('Song Count by Genre in English Lyrics')\n",
    "plt.ylabel('Number of Songs', fontsize=12)\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "6SN5lyQ3Hj_V",
    "outputId": "bd8172e4-75d3-4877-e64e-ad633ae3d35f"
   },
   "outputs": [],
   "source": [
    "sort_df = df.sort_values('words',ascending=False)\n",
    "sort_df = sort_df.iloc[0:9,]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(sort_df.artist, (sort_df.words/sort_df.songcount), alpha=0.8)\n",
    "plt.title('Top 10 Artists by Words/Song')\n",
    "plt.ylabel('Number of Words', fontsize=12)\n",
    "plt.xlabel('Artist', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "colab_type": "code",
    "id": "akeOGZ0_Hj_X",
    "outputId": "d8d92081-137e-4274-eb19-ce38e61ec828"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "grid = plt.GridSpec(1, 3, wspace=0.4, hspace=0.3)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    #plt.subplot(grid[0,i])    #2, 2, i+1)\n",
    "    lyric = \" \".join(lyric for lyric in englishDF[englishDF['Genre']== englishGenres[i]].Lyric)\n",
    "    wordcloud = WordCloud(stopwords=STOPLIST, background_color=\"white\").generate(lyric)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(englishGenres[i] + \" Word Count\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5uH_QJKHj_a"
   },
   "outputs": [],
   "source": [
    "# Print Topics function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g06uLak8Hj_d"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=simple_tokenizer, lowercase=False, min_df=0.01, max_df=0.9)\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, lowercase=False, min_df=0.01, max_df=0.9)\n",
    "\n",
    "# get vectors\n",
    "count_vecs = cv.fit_transform(englishDF['Lyric']).toarray()\n",
    "lyric_features = tfidf.fit_transform(englishDF['Lyric']).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "7ooP0JqnHj_i",
    "outputId": "bcede623-8cde-48be-c6a9-685e15d663fe"
   },
   "outputs": [],
   "source": [
    "\n",
    "# choose the number of components (topics)\n",
    "n_topics = 10\n",
    "n_words = 10\n",
    "\n",
    "# LDA uses word counts\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, n_jobs=-1)\n",
    "lda_lyric_features = lda.fit_transform(count_vecs)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"\\n Topics found via LDA:\")\n",
    "print_topics(lda, cv, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Q2Ym-dGYHj_l",
    "outputId": "4105c0f1-c49c-4180-ef5e-9a8538dd45c6"
   },
   "outputs": [],
   "source": [
    "# get top x words\n",
    "top_words = 10\n",
    "\n",
    "for vectorizer, vecs  in [(cv, count_vecs), (tfidf, lyric_features)]:\n",
    "    for s in [is_rock, ~is_rock]:    \n",
    "        # sum counts\n",
    "        s_sum = vecs[s].sum(axis=0)\n",
    "        # sort arguments\n",
    "        s_sorted = np.argsort(s_sum)\n",
    "        # print top words\n",
    "        print([vectorizer.get_feature_names()[x] for x in s_sorted[-top_words:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DePmD64IHj_o"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "englishDF = finalDF[finalDF['Idiom'] == 'ENGLISH']\n",
    "\n",
    "genreLabels = []\n",
    "\n",
    "for x in englishDF['Genre'].tolist():\n",
    "    if x == 'Rock':\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 0\n",
    "    genreLabels.append(y)\n",
    "\n",
    "is_rock = np.array(genreLabels)\n",
    "\n",
    "\n",
    "lyrics_train, lyrics_test, genre_train, genre_test = train_test_split(englishDF,is_rock, random_state=42, \n",
    "                                             test_size=0.3, shuffle=False)\n",
    "\n",
    "\n",
    "lyrics_train, lyrics_val, genre_train, genre_val = train_test_split(lyrics_train, genre_train, random_state=42,\n",
    "                                             test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKBmp_NwHj_t"
   },
   "outputs": [],
   "source": [
    "def pad_sequence(seqs, seq_len=200):\n",
    "    # function for adding padding to ensure all seq same length\n",
    "    features = np.zeros((len(seqs), seq_len),dtype=int)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if len(seq) != 0:\n",
    "            features[i, -len(seq):] = np.array(seq)[:seq_len]\n",
    "    return features\n",
    "\n",
    "def doc_to_index(docs, vocab):\n",
    "    # transform docs into series of indices\n",
    "    docs_idxs = []\n",
    "    for d in docs:\n",
    "        w_idxs = []\n",
    "        for w in d:\n",
    "            if w in vocab:\n",
    "                w_idxs.append(vocab[w])\n",
    "            else:\n",
    "                # unknown token = 1\n",
    "                w_idxs.append(1)\n",
    "        docs_idxs.append(w_idxs)\n",
    "    return(docs_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rm-TrOBHj_v"
   },
   "outputs": [],
   "source": [
    "parsed_train = [simple_tokenizer(str(d)) for d in lyrics_train['Lyric']]\n",
    "parsed_val = [simple_tokenizer(str(d)) for d in lyrics_val['Lyric']]\n",
    "parsed_test = [simple_tokenizer(str(d)) for d in lyrics_test['Lyric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XOwc1cJJHj_z",
    "outputId": "76e6835d-a6f8-45cb-b630-9e648872d6ed"
   },
   "outputs": [],
   "source": [
    "# construct glove weight matrix\n",
    "# construct vocab\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, ngram_range=(1,1), min_df=0.01, max_df=0.9)\n",
    "cv.fit(parsed_train)\n",
    "vocab = cv.vocabulary_\n",
    "print(\"Size of vocab:\", len(vocab))\n",
    "\n",
    "vocab = dict([(v, vocab[v]+2) for v in vocab])\n",
    "vocab['_UNK'] = 1\n",
    "vocab['_PAD'] = 0\n",
    "glove_vecs = np.zeros(shape=(len(vocab), 300))\n",
    "for k, v in vocab.items():\n",
    "    glove_vecs[v] = nlp(k).vector\n",
    "\n",
    "\n",
    "# additional formatting\n",
    "idx_train = doc_to_index(parsed_train, vocab)\n",
    "padded_train = pad_sequence(idx_train)\n",
    "idx_val = doc_to_index(parsed_val, vocab)\n",
    "padded_val = pad_sequence(idx_val)\n",
    "idx_test = doc_to_index(parsed_test, vocab)\n",
    "padded_test = pad_sequence(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oKlKmDAJHj_2",
    "outputId": "62c362d1-061d-4b54-922e-13af9f6177f7"
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=92, tol=1, max_iter=100000)\n",
    "svc.fit(cv.transform(parsed_train), genre_train)\n",
    "base_accuracy = accuracy_score(genre_test,\n",
    "                               svc.predict(cv.transform(parsed_test)))\n",
    "print(base_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poBqSLoBHj_7"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, embeds, hidden):\n",
    "        # getting the shape information\n",
    "        batch_size, output_len, dimensions = embeds.size()\n",
    "        # attention scores\n",
    "        attention_scores = torch.bmm(embeds, embeds.transpose(1, 2).contiguous())\n",
    "        attention_scores = attention_scores.view(batch_size * output_len, output_len)\n",
    "        # normalize\n",
    "        attention_weights = nn.functional.softmax(attention_scores,dim=1)\n",
    "        attention_weights = attention_weights.view(batch_size, output_len, output_len)\n",
    "        # weight hidden layer\n",
    "        mix = torch.bmm(attention_weights, hidden)\n",
    "        # output result\n",
    "        return mix, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YESqHW4FHj_-"
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    # sentiment classifier with single LSTM layer + Fully-connected layer, sigmoid activation and dropout\n",
    "    # adapted from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "    def __init__(self,\n",
    "                 weight_matrix=None,\n",
    "                 vocab_size=None, \n",
    "                 output_size=1,  \n",
    "                 hidden_dim=512,\n",
    "                 embedding_dim=400, \n",
    "                 n_layers=1, # mistake on previous notebooks, just 1 layer here \n",
    "                 dropout_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        # size of the output, in this case it's one input to one output\n",
    "        self.output_size = output_size\n",
    "        # number of layers (default 2) one LSTM layer, one fully-connected layer\n",
    "        self.n_layers = n_layers\n",
    "        # dimensions of our hidden state, what is passed from one time point to the next\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # initialize the representation to pass to the LSTM\n",
    "        self.embedding, embedding_dim = self.init_embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weight_matrix)\n",
    "        # LSTM layer, where the magic happens\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout_prob, batch_first=True)\n",
    "        # dropout, similar to regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        # sigmoid activiation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # adding attention layer\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # forward pass of the network\n",
    "        batch_size = x.size(0)\n",
    "        # transform input\n",
    "        embeds = self.embedding(x)\n",
    "        # run input embedding + hidden state through model\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # implementing attention\n",
    "        weighted_out, attention_weights = self.attn(embeds, lstm_out)\n",
    "        # using weighted output intstead of lstm output\n",
    "        # reshape\n",
    "        weighted_out = weighted_out.contiguous().view(-1, self.hidden_dim)\n",
    "        # dropout certain pct of connections\n",
    "        out = self.dropout(weighted_out)\n",
    "        # fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # activation function\n",
    "        out = self.sigmoid(out)\n",
    "        # reshape\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        # return the output and the hidden state\n",
    "        return out, hidden, attention_weights, lstm_out\n",
    "    \n",
    "    def init_embedding(self, vocab_size, embedding_dim, weight_matrix):\n",
    "        # initializes the embedding\n",
    "        if weight_matrix is None:\n",
    "            if vocab_size is None:\n",
    "                raise ValueError('If no weight matrix, need a vocab size')\n",
    "            # if embedding is a size, initialize trainable\n",
    "            return(nn.Embedding(vocab_size, embedding_dim),\n",
    "                   embedding_dim)\n",
    "        else:\n",
    "            # otherwise use matrix as pretrained\n",
    "            weights = torch.FloatTensor(weight_matrix)\n",
    "            return(nn.Embedding.from_pretrained(weights),\n",
    "                  weights.shape[1])\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # initializes the hidden state\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAnsW5OsHkAB"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, model_params, training_params):\n",
    "    # utility for running the training process\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=training_params['learning_rate'])\n",
    "    epochs = training_params['epochs']\n",
    "    batch_size = training_params['batch_size']\n",
    "    # print options\n",
    "    counter = 0\n",
    "    print_every = 5\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            ###############Bug fix code for CPU on windows####################\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###############Bug fix code####################\n",
    "        \n",
    "            model.zero_grad()\n",
    "            output, h, train_attention_weights, lstm_out = model(inputs, h)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if counter%print_every == 0:\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inp, lab in val_loader:\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inp, lab = inp.to(device), lab.to(device)\n",
    "                    \n",
    "                    ###############Bug fix code for CPU on windows####################\n",
    "                    inp = inp.type(torch.LongTensor)\n",
    "                    lab = lab.type(torch.LongTensor)\n",
    "\n",
    "                    inp = inp.to(device)\n",
    "                    lab = lab.to(device)\n",
    "                    ###############Bug fix code####################\n",
    "                \n",
    "                    out, val_h, val_attention_weights, _ = model(inp, val_h)\n",
    "                    val_loss = criterion(out.squeeze(), lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                #print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                      #\"Step: {}...\".format(counter),\n",
    "                      #\"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      #\"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), './state_dict.pt')\n",
    "                    #print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                    valid_loss_min = np.mean(val_losses)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1adkOodHkAE"
   },
   "outputs": [],
   "source": [
    "def assess_accuracy(model, test_loader, model_params, training_params):\n",
    "    # utility for assessing accuracy\n",
    "    batch_size = training_params['batch_size']\n",
    "    model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "    h = model.init_hidden(batch_size)\n",
    "    num_correct = 0\n",
    "    model.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        h = tuple([each.data for each in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        ###############Bug fix code for CPU on windows####################\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ###############Bug fix code####################\n",
    "    \n",
    "        #output, h = model(inputs, h)\n",
    "        output, h, train_attention_weights, lstm_out = model(inputs, h)\n",
    "        # takes output, rounds to 0/1\n",
    "        pred = torch.round(output.squeeze())\n",
    "        # take the correct labels, check against preds\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # sum the number of correct\n",
    "        num_correct += np.sum(correct)\n",
    "    # calc accuracy\n",
    "    test_acc = num_correct/len(test_loader.dataset)\n",
    "    print('LSTM accuracy:', test_acc)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0pac308HkAH"
   },
   "outputs": [],
   "source": [
    "\n",
    "# construct datasets for loading by PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(padded_train), torch.from_numpy(genre_train))\n",
    "val_data = TensorDataset(torch.from_numpy(padded_val), torch.from_numpy(genre_val))\n",
    "test_data = TensorDataset(torch.from_numpy(padded_test), torch.from_numpy(genre_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "JMrg__xsHkAK",
    "outputId": "9824045c-f97b-4333-e733-97a9f00e2963"
   },
   "outputs": [],
   "source": [
    "# initialize array for the variables to iterate\n",
    "\n",
    "epoch = [5,10,15]\n",
    "dropout = [0.2]\n",
    "learning = [0.004]\n",
    "batch = [100]\n",
    "\n",
    "cols = ['epoch', 'dropout', 'learning', 'batch', 'accuracy']\n",
    "resultsDF = pd.DataFrame(columns = cols)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for x, y, z, b in [(x,y,z,b) for x in batch for y in dropout for z in learning for b in epoch]:\n",
    "    # construct datasets for loading by PyTorch\n",
    "\n",
    "    batch_size = x\n",
    "\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,\n",
    "                         drop_last=True) # this is to keep the size consistent\n",
    "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size,\n",
    "                       drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,\n",
    "                        drop_last=True)\n",
    "    \n",
    "    model_params = {'weight_matrix': glove_vecs,\n",
    "               'output_size': 1,\n",
    "               'hidden_dim': 512,\n",
    "               'n_layers': 2,\n",
    "               'embedding_dim': 400,\n",
    "               'dropout_prob': y}\n",
    "    model = SentimentNet(**model_params)\n",
    "    \n",
    "    training_params = {'learning_rate': z,\n",
    "                      'epochs': b,\n",
    "                      'batch_size': batch_size}\n",
    "    train_model(model,train_loader, val_loader, model.parameters, training_params)\n",
    "    \n",
    "\n",
    "    print(\"\\n Batch: {}...\".format(x),\n",
    "          \"Dropout Rate: {}...\".format(y),\n",
    "          \"Learning Rate: {}...\".format(z),\n",
    "          \"Epochs: {}\".format(b))\n",
    "    \n",
    "                \n",
    "    acc = assess_accuracy(model,train_loader, test_loader,  training_params)\n",
    "    \n",
    "    resultsDF.loc[i,'epoch'] = b\n",
    "    resultsDF.loc[i,'dropout'] = y\n",
    "    resultsDF.loc[i,'learning'] = z\n",
    "    resultsDF.loc[i,'batch'] = x\n",
    "    resultsDF.loc[i,'accuracy'] = acc\n",
    "    \n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "gCxr10TjMnNO",
    "outputId": "0c68fd93-1244-437b-dba2-4198efc67d31"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(resultsDF['epoch'], resultsDF['accuracy'])\n",
    "plt.title('Sample=109000, Batch=100, DropoutRate=0.2, LearningRare=0.004')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "6kl-O8R_HkAM",
    "outputId": "91040372-95b1-4fb2-d2af-28f1f838652f"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(resultsDF['epoch'], resultsDF['accuracy'])\n",
    "plt.title('Sample=10000, Batch=100, DropoutRate=0.2, LearningRare=0.004')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "bhyrCwLgUUAm",
    "outputId": "38485c77-c3f2-4dfc-9fd4-2fbf79d147c1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(resultsDF['epoch'], resultsDF['accuracy'])\n",
    "plt.title('Sample=5000, Batch=100, DropoutRate=0.2, LearningRare=0.004')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "A1rDsT2nHkAP",
    "outputId": "a82a0bbf-1122-4910-cd40-525f9b23eb96"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(resultsDF['epoch'], resultsDF['accuracy'])\n",
    "plt.title('Sample=1000, Batch=100, DropoutRate=0.2, LearningRare=0.004')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LyricsAnalysisGenrePredictionLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
